# MT-GBM（Multi-task Gradient Boosting Machine）全体アルゴリズム

このアルゴリズムは、複数のタスクから得られる情報を統合し、共有された決定木の構造を学習するプロセスを記述します。

### 1. 初期化

まず、データセットの全サンプル $x_i$ に対して、初期予測値 $F_0(x_i)$ を決定します。通常、各タスクの目的変数（ラベル）の平均値などが用いられます。

$$F_0(x_i) = \text{argmin}_c \sum_{i=1}^{N} L(y_i, c)$$

### 2. 木の構築ループ

$m=1$ から $M$（木の総数）まで、以下のステップを繰り返します。

#### ステップ A：勾配・ヘシアンの計算

現在のモデル全体の予測値 $F_{m-1}(x_i)$ を基に、データセットの全サンプル $i$ に対して、各タスク $j \in \{1, ..., n\}$ の勾配 $G_{ij}$ とヘシアン $H_{ij}$ を計算します。

$$G_{ij} = \left[ \frac{\partial L(y_{ij}, F)}{\partial F} \right]_{F=F_{m-1}(x_i)}$$
$$H_{ij} = \left[ \frac{\partial^2 L(y_{ij}, F)}{\partial F^2} \right]_{F=F_{m-1}(x_i)}$$

#### ステップ B：分岐用・更新用情報の生成

ステップAで計算した生の勾配・ヘシアンを用いて、以下の2種類の情報を生成します。

1.  **分岐用**: **アルゴリズム2（後述）** を実行し、木の分岐決定に用いる**アンサンブル勾配・ヘシアン ($G_{ie}, H_{ie}$) を全サンプル分、生成します。
2.  **更新用**: **アルゴリズム3（後述）** を実行し、葉の出力値の計算に用いる**更新用勾配・ヘシアン ($G_{iju}, H_{iju}$)** を全サンプル分、生成します。

#### ステップ C：新しい決定木 ($T_m$) の構築

ステップBで生成した**分岐用の$G_e, H_e$のみ**を使って、新しい木を構築します。

* ルートノードから再帰的に、以下の情報利得（Gain）を最大化する分岐を探します。
$$\text{Gain} = \frac{1}{2} \left[ \frac{G_{eL}^2}{H_{eL} + \lambda} + \frac{G_{eR}^2}{H_{eR} + \lambda} - \frac{(G_{eL} + G_{eR})^2}{H_{eL} + H_{eR} + \lambda} \right] - \gamma$$
* $G_{eL}, H_{eL}$ などは、それぞれ左の子ノードに属するサンプルたちの $G_e, H_e$ の合計値です。
* 木の深さなどの停止条件を満たすまで、ノードの分割を繰り返します。

#### ステップ D：葉の出力値の決定

木の構造が完成したら、各葉ノードの出力値を計算します。

* 各葉ノード $k$ について、そこに属するサンプルたちの**更新用の$G_{un}, H_{un}$** をタスクごとに合計します。
* タスク $j$ に対する葉 $k$ の出力値 $output_{kj}$ は以下のように計算されます。
$$output_{kj} = - \frac{\sum_{i \in \text{leaf}_k} G_{iju}}{\sum_{i \in \text{leaf}_k} H_{iju} + \lambda}$$

#### ステップ E：モデルの更新

新しく作られた木 $T_m$ の予測値に学習率 $\eta$ を掛け、現在のモデル $F_{m-1}$ に加算して、モデル全体を更新します。

$$F_m(x) = F_{m-1}(x) + \eta T_m(x)$$

---
---

## 付属アルゴリズム

### アルゴリズム2：アンサンブル勾配・ヘシアンの計算

**目的**: 全タスクの情報を統合し、木の分岐決定に用いる単一の勾配・ヘシアン ($G_e, H_e$) を生成する。
**入力**: 各タスクの勾配ベクトル $G_1, ..., G_n$、ヘシアンベクトル $H_1, ..., H_n$
**出力**: アンサンブル勾配ベクトル $G_e$、アンサンブルヘシアンベクトル $H_e$

1.  **勾配の正規化重み ($w_j$) の決定**:
    各タスク $j$ について、スケーリングされた勾配ベクトル $w_j G_j$ が、平均`0.05`、標準偏差`0.01`となるような重み $w_j$ を決定します。

2.  **勾配の重みの強調**:
    タスクのインデックス集合 $\{1, ..., n\}$ から、強調したいタスクのインデックス集合 $K$ をランダムに選択します。選択されたタスクの重みを、ハイパーパラメータ $\gamma$ （10～100の値）で増幅させます。
    $$
    w'_j = 
    \begin{cases} 
    \gamma \cdot w_j & \text{if } j \in K \\
    w_j & \text{if } j \notin K 
    \end{cases}
    $$

3.  **アンサンブル勾配の計算**:
    $$
    G_e = \sum_{j=1}^{n} w'_j G_j
    $$

4.  **ヘシアンの正規化重み ($v_j$) の決定**:
    各タスク $j$ について、スケーリングされたヘシアンベクトル $v_j H_j$ が、平均`1.0`、標準偏差`0.1`となるような重み $v_j$ を決定します。

5.  **アンサンブルヘシアンの計算**:
    $$
    H_e = \sum_{j=1}^{n} v_j H_j
    $$

### アルゴリズム3：更新用勾配・ヘシアンの計算

**目的**: 葉の出力値の計算に用いるための、学習ペースが調整された勾配・ヘシアン ($G_{un}, H_{un}$) を生成する。
**入力**: 各タスクの勾配ベクトル $G_1, ..., G_n$、ヘシアンベクトル $H_1, ..., H_n$
**出力**: 更新用勾配ベクトル $G_{u1}, ..., G_{un}$、更新用ヘシアンベクトル $H_{u1}, ..., H_{un}$

1.  **初期値の設定**:
    更新用勾配・ヘシアンの初期値を、元の勾配・ヘシアンと同じにします。
    $$
    G_{uj} = G_j, \quad H_{uj} = H_j \quad (\text{for } j=1, ..., n)
    $$

2.  **勾配ベクトル間の相関 `corr` の計算**:
    全タスクの勾配ベクトル $\{G_1, ..., G_n\}$ 間の相関の度合いを、何らかの関数 $f$ で計算し、単一のスカラー値 `corr` を得ます。

3.  **相関による勾配の調整**:
    全タスクの更新用勾配 $G_{uj}$ に、計算された相関 `corr` を乗算します。`corr` は`0.5`から`1`の間にクリッピング（丸め込み）されます。
    $$
    G_{uj} \leftarrow G_{uj} \times \text{clip}(\text{corr}, 0.5, 1.0) \quad (\text{for } j=1, ..., n)
    $$
    （※元論文ではヘシアンへの操作は明記されていませんが、更新用として $H_{uj}$ もペアで扱われます）

- 研究背景（＞－＜形式を意識）
  - 導入：CVR予測の有用性：ECサイトやプロダクトプラットフォーム、オンライン広告においてCVR予測を行うことでマーケティングや推薦システムに活かしている
  - 今回の研究概要：コンバージョンファネル*の構造を活用した、CVR予測手法の提案
    - CVファネルの図、CVRを予測しますの図
    - ECサイトだけでなく、Youtubeをはじめとするプロダクトプラットフォームやオンライン広告配信サービス(RTB)で見られる報酬構造
  - 課題：実応用の場面では目的変数であるCVにクラス間不均衡や0過剰がある場合が良く見られ、推定精度の低下に悩まされている
    - アンダー|オーバーサンプリングや損失関数の工夫など、さまざまな対策が取られてきた
  - ところで、ECサイトにおいては Impression → Click → CV と、真に推定したい目的変数であるCVの手前に、ImpressionやClickなどの補助変数が存在する。
    - 一般にCV = 1 → Click = 1, Impression = 1 であり、また一定期間のClick数とCV数には強い相関がある。
  - 主に深層学習モデルにおいてこのフェーズを統合的にモデリングするような手法 (Alibaba2018, ?2024)は提案されているが、表形式データを扱う上で簡便かつ優れた性能を残すGBDTモデルにおいて、そのような手法は提案されていない
  - 目的：最終的なCVを表形式データに有効なGBDTで推定するにあたり、ImpressionやClickの情報を補助的に活かすことで、不均衡データであるCVの予測精度を向上させる

# アンサンブル重み実装計画書
- アンサンブル重み設定タイミングに関して，ハイパーパラメータ `is_dynamic_weight` で制御
  - `is_dynamic_weight == False` なら，アンサンブル重みは弱学習器1つ1つのイテレーション前に決定（m個の弱学習器がある場合，m回決定・定義する）
    - 具体的な決定方法は以下の通り
      - まず，各タスクの勾配・ヘシアンについて，平均0.05，標準偏差0.01とするような正規化を行うことの出来るアンサンブル重みを各弱学習器の学習前に計算し，各タスクの勾配・ヘシアンに乗算する
      - 更に，ランダムにハイパーパラメータ `gamma` (デフォルトは50) をどれか1つのタスクの勾配に乗算する
  - `is_dynamic_weight == True` なら，アンサンブル重みは弱学習器の学習における各分岐構造決定時（情報利得最大の計算）を行う前に決定（仮にm個の弱学習器において分岐数がBなら，m*B回決定・定義する）
    - 具体的な決定方法は以下の通り
      - まず，各タスクの勾配・ヘシアンについて，平均0.05，標準偏差0.01とするような正規化を行うことの出来るアンサンブル重みを各弱学習器の学習前に計算し，各タスクの勾配・ヘシアンに乗算する（ここまでは静的重みづけと変わらず）
      - 更に，ランダムにハイパーパラメータ `gamma` (デフォルトは50) はまずCVR予測タスクの勾配に乗算する（CVR予測タスクとCTR予測タスクの2タスクマルチタスク学習だけを前提としていいです．必ずインデックス0にCVR予測タスク，インデックス1にCTR予測タスクが来るとして良いです．）
      - 再帰的に最適分岐構造を決定すると思いますが，その際にアンサンブル勾配・ヘシアンにより得られた最大情報利得の値が，ハイパーパラメータ `delta` (デフォルト値は0.5) を下回る場合に，アンサンブル重みの計算からやり直します．データのアンサンブル勾配やヘシアンは既に計算済でどこかの変数に保存されていると思いますが，親ノードに属するデータだけ別変数に抽出して，CVRタスクの勾配を $1 / \gamma$ 倍，CTRタスクの勾配を $\gamma$ 倍するようにして下さい．
      - このようにして新たに計算されたアンサンブル勾配・ヘシアンを用いて，改めて最大情報利得の分岐点を探索し，分岐構造を確定するようにしてください．
# Mゼミ反映
- CVファネルは相関があるというより，もはや階層構造で順序変数としてみなせる
- 藤原に関連論文聞いてみる
- CVRは条件付きにすると良さそう，また諸問題が発生しそうですけど，うまく実装できそうかな．
## 今後のタスク
- [ ] 藤原に論文聞く
- [ ] 提案手法実装
- [ ] 比較手法実装
  - [ ] Clickも特徴量に含める単一 vs MT-GBM
- [ ] GBDTの各分岐におけるCV率・CVRのみの情報利得ログを取得する
  - [ ] 深さを用いて層別し，判断材料とする 