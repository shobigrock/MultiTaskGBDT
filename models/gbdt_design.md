# MT-GBM（Multi-task Gradient Boosting Machine）全体アルゴリズム

このアルゴリズムは、複数のタスクから得られる情報を統合し、共有された決定木の構造を学習するプロセスを記述します。

### 1. 初期化

まず、データセットの全サンプル $x_i$ に対して、初期予測値 $F_0(x_i)$ を決定します。通常、各タスクの目的変数（ラベル）の平均値などが用いられます。

$$F_0(x_i) = \text{argmin}_c \sum_{i=1}^{N} L(y_i, c)$$

### 2. 木の構築ループ

$m=1$ から $M$（木の総数）まで、以下のステップを繰り返します。

#### ステップ A：勾配・ヘシアンの計算

現在のモデル全体の予測値 $F_{m-1}(x_i)$ を基に、データセットの全サンプル $i$ に対して、各タスク $j \in \{1, ..., n\}$ の勾配 $G_{ij}$ とヘシアン $H_{ij}$ を計算します。

$$G_{ij} = \left[ \frac{\partial L(y_{ij}, F)}{\partial F} \right]_{F=F_{m-1}(x_i)}$$
$$H_{ij} = \left[ \frac{\partial^2 L(y_{ij}, F)}{\partial F^2} \right]_{F=F_{m-1}(x_i)}$$

#### ステップ B：分岐用・更新用情報の生成

ステップAで計算した生の勾配・ヘシアンを用いて、以下の2種類の情報を生成します。

1.  **分岐用**: **アルゴリズム2（後述）** を実行し、木の分岐決定に用いる**アンサンブル勾配・ヘシアン ($G_{ie}, H_{ie}$) を全サンプル分、生成します。
2.  **更新用**: **アルゴリズム3（後述）** を実行し、葉の出力値の計算に用いる**更新用勾配・ヘシアン ($G_{iju}, H_{iju}$)** を全サンプル分、生成します。

#### ステップ C：新しい決定木 ($T_m$) の構築

ステップBで生成した**分岐用の$G_e, H_e$のみ**を使って、新しい木を構築します。

* ルートノードから再帰的に、以下の情報利得（Gain）を最大化する分岐を探します。
$$\text{Gain} = \frac{1}{2} \left[ \frac{G_{eL}^2}{H_{eL} + \lambda} + \frac{G_{eR}^2}{H_{eR} + \lambda} - \frac{(G_{eL} + G_{eR})^2}{H_{eL} + H_{eR} + \lambda} \right] - \gamma$$
* $G_{eL}, H_{eL}$ などは、それぞれ左の子ノードに属するサンプルたちの $G_e, H_e$ の合計値です。
* 木の深さなどの停止条件を満たすまで、ノードの分割を繰り返します。

#### ステップ D：葉の出力値の決定

木の構造が完成したら、各葉ノードの出力値を計算します。

* 各葉ノード $k$ について、そこに属するサンプルたちの**更新用の$G_{un}, H_{un}$** をタスクごとに合計します。
* タスク $j$ に対する葉 $k$ の出力値 $output_{kj}$ は以下のように計算されます。
$$output_{kj} = - \frac{\sum_{i \in \text{leaf}_k} G_{iju}}{\sum_{i \in \text{leaf}_k} H_{iju} + \lambda}$$

#### ステップ E：モデルの更新

新しく作られた木 $T_m$ の予測値に学習率 $\eta$ を掛け、現在のモデル $F_{m-1}$ に加算して、モデル全体を更新します。

$$F_m(x) = F_{m-1}(x) + \eta T_m(x)$$

---
---

## 付属アルゴリズム

### アルゴリズム2：アンサンブル勾配・ヘシアンの計算

**目的**: 全タスクの情報を統合し、木の分岐決定に用いる単一の勾配・ヘシアン ($G_e, H_e$) を生成する。
**入力**: 各タスクの勾配ベクトル $G_1, ..., G_n$、ヘシアンベクトル $H_1, ..., H_n$
**出力**: アンサンブル勾配ベクトル $G_e$、アンサンブルヘシアンベクトル $H_e$

1.  **勾配の正規化重み ($w_j$) の決定**:
    各タスク $j$ について、スケーリングされた勾配ベクトル $w_j G_j$ が、平均`0.05`、標準偏差`0.01`となるような重み $w_j$ を決定します。

2.  **勾配の重みの強調**:
    タスクのインデックス集合 $\{1, ..., n\}$ から、強調したいタスクのインデックス集合 $K$ をランダムに選択します。選択されたタスクの重みを、ハイパーパラメータ $\gamma$ （10～100の値）で増幅させます。
    $$
    w'_j = 
    \begin{cases} 
    \gamma \cdot w_j & \text{if } j \in K \\
    w_j & \text{if } j \notin K 
    \end{cases}
    $$

3.  **アンサンブル勾配の計算**:
    $$
    G_e = \sum_{j=1}^{n} w'_j G_j
    $$

4.  **ヘシアンの正規化重み ($v_j$) の決定**:
    各タスク $j$ について、スケーリングされたヘシアンベクトル $v_j H_j$ が、平均`1.0`、標準偏差`0.1`となるような重み $v_j$ を決定します。

5.  **アンサンブルヘシアンの計算**:
    $$
    H_e = \sum_{j=1}^{n} v_j H_j
    $$

### アルゴリズム3：更新用勾配・ヘシアンの計算

**目的**: 葉の出力値の計算に用いるための、学習ペースが調整された勾配・ヘシアン ($G_{un}, H_{un}$) を生成する。
**入力**: 各タスクの勾配ベクトル $G_1, ..., G_n$、ヘシアンベクトル $H_1, ..., H_n$
**出力**: 更新用勾配ベクトル $G_{u1}, ..., G_{un}$、更新用ヘシアンベクトル $H_{u1}, ..., H_{un}$

1.  **初期値の設定**:
    更新用勾配・ヘシアンの初期値を、元の勾配・ヘシアンと同じにします。
    $$
    G_{uj} = G_j, \quad H_{uj} = H_j \quad (\text{for } j=1, ..., n)
    $$

2.  **勾配ベクトル間の相関 `corr` の計算**:
    全タスクの勾配ベクトル $\{G_1, ..., G_n\}$ 間の相関の度合いを、何らかの関数 $f$ で計算し、単一のスカラー値 `corr` を得ます。

3.  **相関による勾配の調整**:
    全タスクの更新用勾配 $G_{uj}$ に、計算された相関 `corr` を乗算します。`corr` は`0.5`から`1`の間にクリッピング（丸め込み）されます。
    $$
    G_{uj} \leftarrow G_{uj} \times \text{clip}(\text{corr}, 0.5, 1.0) \quad (\text{for } j=1, ..., n)
    $$
    （※元論文ではヘシアンへの操作は明記されていませんが、更新用として $H_{uj}$ もペアで扱われます）